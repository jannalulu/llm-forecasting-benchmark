2024-11-06 13:22:12,028 - INFO - Question: Will three or more Frontier AI Labs issue a joint statement committing to constrain their AI's capabilities before 2025?

2024-11-06 13:22:12,030 - INFO - News articles for question 29588:
Here are the relevant news articles:

**Meta Permits Its AI Models To Be Used For US Military Purposes**
Meta has changed its policy to allow its artificial intelligence models, called Llama, to be used for military purposes by U.S. government agencies and contractors. This shift is an exception to Meta's 'acceptable use policy,' which previously prohibited the use of its technology for military and warfare purposes. According to Nick Clegg, Meta's president of global affairs, the company now supports 'responsible and ethical uses' of the technology that align with 'democratic values' and the safety and security of America and its allies. Meta will share its technology with members of the Five Eyes intelligence alliance, including Canada, Britain, Australia, New Zealand, and the United States. 'Meta wants to play its part to support the safety, security and economic prosperity of America -- and of its closest allies too,' Mr. Clegg wrote. 'Widespread adoption of American open source A.I. models serves both economic and security interests.'
Original language: en
Publish date: November 05, 2024 08:00 AM
Source:[upstract.com](https://news.slashdot.org/story/24/11/05/043209/meta-permits-its-ai-models-to-be-used-for-us-military-purposes?ref=upstract.com)

**OpenAI's Sam Altman: Achieving General Artificial Intelligence is Possible, but Challenging**
Sam Altman, CEO of OpenAI, stated during a Reddit AMA session that achieving general artificial intelligence (AGI) is possible using current available hardware. However, he also mentioned that achieving this goal may take years and would require building 36 new semiconductor factories and additional data centers at a cost of around $7 trillion. Altman expressed his enthusiasm for the upcoming new releases this year, but denied rumors about the imminent launch of GPT-5, stating that the company has no plans to launch a model with this name soon. He also denied recent reports about the upcoming launch of a new model called 'Orion' by December of this year, calling them 'false news'. Kevin Wailes, OpenAI's head of products, mentioned that ChatGPT will see improvements in performance, with the addition of more GPUs to support its advanced capabilities. Altman hinted at a new feature that will be added to ChatGPT soon, allowing for automatic completion of voice conversations. OpenAI stated that 2025 will be a turning point, as the company plans to develop ChatGPT to be able to perform tasks independently using 'artificial intelligence agents', a type of AI that can perform tasks autonomously without continuous human intervention, and make decisions on behalf of the user or another system, with features such as environmental awareness, task execution, and learning from experiences to improve performance over time.
Original language: ar
Publish date: November 04, 2024 09:19 PM
Source:[gulfnow.com](https://gulfnow.org/technology/242098/%D8%A8%D8%AA%D9%83%D9%84%D9%81%D8%A9-%D8%AE%D9%8A%D8%A7%D9%84%D9%8A%D8%A9-%D8%B3%D8%A7%D9%85-%D8%A3%D9%84%D8%AA%D9%85%D8%A7%D9%86-%D9%8A%D8%B4%D9%8A%D8%B1-%D8%A5%D9%84%D9%89-%D8%A5%D9%85%D9%83%D8%A7%D9%86%D9%8A%D8%A9-%D8%A7%D9%84%D9%88%D8%B5%D9%88%D9%84-%D8%A5%D9%84%D9%89-%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1-%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A-%D8%A7%D9%84%D8%B9%D8%A7%D9%85---%D9%85%D9%88%D9%82%D8%B9-%D8%A7%D9%84%D8%AE%D9%84%D9%8A%D8%AC-%D8%A7%D9%84%D8%A7%D9%86.html)

**OPPO and HKPolyU Renew Collaboration and Launch Joint Innovation Research Centre to Expand AI Imaging Frontiers**
OPPO and The Hong Kong Polytechnic University (PolyU) have renewed their collaboration to further deepen their partnership in AI imaging technology. The agreement, signed on November 1, 2024, will see OPPO invest at least RMB 30 million over the next five years to expand the scale of co-trained PhD and postdoctoral researchers. According to Jason LIAO, President of OPPO Research Institute, 'In the three years of collaboration between OPPO and PolyU, we have witnessed significant technological breakthroughs and notable achievements in talent cultivation through the Joint Lab.' Prof. Jin-Guang TENG, President of PolyU, added, 'To address the opportunities and challenges of the AI era, PolyU will officially establish the Faculty of Computer and Mathematical Sciences in January 2025, aiming to meet growing technological needs and support talent nurturing.' The collaboration has already achieved several innovative milestones, including the development of AI super-resolution technology and HDR imaging algorithms, which have been applied to multiple OPPO products.
Original language: en
Publish date: November 04, 2024 10:09 AM
Source:[lankabusinessnews.com](https://www.lankabusinessnews.com/oppo-and-hkpolyu-renew-collaboration-and-launch-joint-innovation-research-centre-to-expand-ai-imaging-frontiers/)

**Meta's Llama 4: Training with Over 100,000 NVIDIA GPUs**
Meta's CEO Mark Zuckerberg announced that Llama 4 will be available in early 2025. The model was trained using a cluster of over 100,000 NVIDIA H100 GPUs, a significant increase from the 25,000 GPUs used to train Llama 3. According to Zuckerberg, Llama 4 will be faster, support new modes, and have better reasoning capabilities. The first variant of the model is expected to be ready in early 2025. However, the enormous power required to train the model, estimated to be over 150 MW, raises concerns about energy consumption. In comparison, the supercomputer El Capitain consumes only 30 MW, and Frontier, the first-ranked supercomputer in the TOP500, consumes less than 23 MW. Zuckerberg did not provide further details on the model, stating only that it will be more efficient and have better capabilities. 'It will be more powerful and support new features,' Zuckerberg said. 'We are committed to making AI more accessible and useful for everyone.' 
Original language: it
Publish date: November 04, 2024 12:00 AM
Source:[punto-informatico.it](https://www.punto-informatico.it/meta-llama-4-training-con-oltre-100-000-gpu-nvidia/)

**A venture capitalist on where the AI opportunities are for investors**
Martin Casado, a general partner at Andreessen Horowitz, discussed the current state of artificial intelligence (AI) with The Wall Street Journal's global technology editor, Jason Dean. Casado emphasized that the marginal cost of language, reasoning, and creation are going to zero, making this a 'supercycle' with decades of potential. He highlighted three use cases that are working well: creative composition, companionship, and code. Casado also noted that AI can produce creative works, but they are not a replacement for human artists. He also discussed the limitations of AI, including the need for data and the potential for slowing down as we run out of existing knowledge. Finally, he emphasized that even if frontier models stop advancing, there is still value to be had in turning them into applications for people to use.
Original language: en
Publish date: October 28, 2024 07:52 AM
Source:[mint](https://www.livemint.com/ai/artificial-intelligence/a-venture-capitalist-on-where-the-ai-opportunities-are-for-investors-11730101538580.html)

**2024 outlook for the top 10 AI frontier technology trends unveiled**
The 2024 outlook for the top 10 AI frontier technology trends was unveiled at the World Science and Technology Development Forum in Beijing, led by Qiao Hong, chairman of the World Robot Cooperation Organization. The trends highlighted AI ethics issues, with experts emphasizing the need for an AI supervision model framework, clear standards, and regulations to ensure AI systems adhere to established principles. The trends also include interpretable models, full-modal large models, embodied small brain models, and world simulators, among others. According to Qiao, these technologies 'are full of infinite possibilities and potential, not only bringing a more convenient and efficient way of life but also driving innovation and development across various industries.' 
Original language: en
Publish date: October 28, 2024 06:13 AM
Source:[China Daily](https://www.chinadaily.com.cn/a/202410/28/WS671f2b47a310f1265a1c9fea.html)

**OpenAI Senior Advisor Joins Exodus, Warns 'Neither OpenAI Nor Any Other Frontier Lab Is Ready' For AGI**
Miles Brundage, OpenAI's senior advisor for AGI Readiness, has resigned, warning that neither OpenAI nor any other AI company is prepared for the development of artificial general intelligence. Brundage emphasized concerns about the industry's preparedness for advanced AI development, stating, 'Neither OpenAI nor any other frontier lab is ready, and the world is also not ready.' He plans to launch a nonprofit focused on AI policy research and advocacy, and expressed concerns about the broader AI industry's approach to safety and security, particularly regarding systems capable of posing catastrophic risks. Brundage stated, 'I think AI capabilities are improving very quickly and policymakers need to act more urgently,' emphasizing the need for enhanced regulation and safety measures across the AI industry.
Original language: en
Publish date: October 25, 2024 09:17 AM
Source:[Benzinga](https://www.benzinga.com/news/24/10/41538918/openai-senior-advisor-joins-exodus-warns-neither-openai-nor-any-other-frontier-lab-is-ready-for-agi)

**Statement from National Economic Advisor Lael Brainard on National Security Memorandum (NSM) on Artificial Intelligence (AI) | The White House**
The White House has issued the first-ever National Security Memorandum (NSM) on Artificial Intelligence (AI), which sets out goals to enable the US Government to harness cutting-edge AI technologies and advance international consensus and governance around AI. National Economic Advisor Lael Brainard stated, 'The US lead today on the most advanced AI models reflects several important US economic strengths: our innovative private sector, the ability to develop and source world-class talent, strengths in advanced semiconductor design, dynamic capital allocation, and abundant compute power.' However, Brainard emphasized that these strengths should not be taken for granted in the future and that sustaining US preeminence in frontier AI will require strong domestic foundations in semiconductors, infrastructure, and clean energy. The NSM directs the National Economic Council to coordinate an economic assessment of the relative competitive advantage of the US private sector AI ecosystem and aims to accelerate the deployment of transmission and clean energy projects to meet the needs of AI datacenters. Brainard noted that the historic Biden-Harris investment laws will be critical enablers and that the government is committed to helping navigate permitting processes across the federal government, working with states and localities, and ensuring AI infrastructure creates good jobs while investing in the workforce to enable American workers to drive innovation.
Original language: en
Publish date: October 24, 2024 10:10 AM
Source:[The White House](https://www.whitehouse.gov/briefing-room/statements-releases/2024/10/24/statement-from-national-economic-advisor-lael-brainard-on-national-security-memorandum-nsm-on-artificial-intelligence-ai/)

**AI labs are fragmenting because of ego clashes and commercial pressures. It's about to drive a wave of new startups.**
The high-stakes AI lab landscape is fragmenting due to ego clashes, commercial pressures, and personal values. Researchers are leaving large labs to form new startups, attracting significant investments from VCs. For example, Ilya Sutskever's Safe Superintelligence raised $1 billion, while Black Forest Labs and H secured $100 million and $220 million respectively. According to PitchBook, startups building foundational models have raised a record $22.9 billion in VC funding so far in 2024. The trend is expected to continue, with more AI labs fragmenting and new startups emerging. Noel Hurley, ex-VP at Arm, attributes this phenomenon to 'ego-driven' researchers who are 'told they're brilliant' and encouraged by the VC community. Nathan Benaich, founder of Air Street Capital, notes that researchers may want to take their work in different directions, leading to a 'cracked team' that can achieve more with fewer people. Samir Kumar, general partner at Touring Capital, expects waves of new companies to emerge at the frontier of the gen AI shift, as talent seeks more control and flexibility.
Original language: en
Publish date: October 15, 2024 09:55 AM
Source:[Business Insider](https://www.businessinsider.com/ai-labs-fragmenting-ego-clashes-new-startups-openai-2024-10)

**September 2024 Developments Under President Biden's AI Executive Order**
In September 2024, the US government made significant progress in implementing President Biden's AI Executive Order. The Bureau of Industry and Security proposed updated technical thresholds for dual-use foundation model reporting requirements, which would require developers to report AI models trained using more than 10 FLOPS of computing power. The GAO released a report finding that several agencies have fully implemented AI management and talent requirements from the AI EO. The US Commerce and State Departments announced plans to host the inaugural meeting of the International Network of AI Safety Institutes, bringing together technical AI experts from member countries to advance global collaboration and knowledge sharing on AI safety. At least 31 federal agencies have published AI compliance plans, including GSA, SEC, EEOC, NASA, and the Departments of Defense, State, Homeland Security, Commerce, and Treasury. California Governor Gavin Newsom vetoed the Safe and Secure Innovation for Frontier AI Models Act, citing concerns that the bill's thresholds were too stringent and could give the public a false sense of security.
Original language: en
Publish date: October 14, 2024 04:39 AM
Source:[Lexology](https://www.lexology.com/library/detail.aspx?g=932d8c10-5ce0-49c3-b382-7130b15055b9)

**Newsom throws AI regulation fight into uncertainty with veto**
California Governor Gavin Newsom's veto of the Safe and Secure Innovation for Frontier Artificial Intelligence Models Act (SB 1047) has sparked debate over the regulation of artificial intelligence (AI). The bill, which aimed to require safety testing for powerful AI models, was seen as a 'first-of-its-kind legislation' by some, but Newsom argued it was too broad and didn't take into account the specific risks of AI deployment. Kaili Lambe, policy and advocacy director for Accountable Tech, expressed concern that regulation often moves slowly while technology advances rapidly. Landon Klein, director of U.S. policy for the Future of Life Institute, agreed that timely regulation is necessary to prevent risks associated with AI. Newsom has proposed partnering with industry experts to develop guardrails for AI, but some experts, such as Daniel Castro, vice president at the Information and Innovation Foundation, argue that more evidence is needed before placing guardrails on the tech. The debate over AI regulation is ongoing, with some advocating for open-source AI and others calling for more transparency and understanding of the technology.
Original language: en
Publish date: October 02, 2024 10:00 AM
Source:[The Hill](https://thehill.com/policy/technology/4910138-newsom-throws-ai-regulation-fight-into-uncertainty-with-veto)

**California's Missed Opportunity in AI Security: Newsom's Veto of SB 1047**
California Governor Gavin Newsom recently vetoed the 1047 California Artificial Intelligence Security Act (SB-1047 Safe and Secure Innovation for Frontier Artificial Intelligence Models Act), which aimed to make strong AI models subject to mandatory security testing before being released to the public. The bill, proposed by Democratic State Senator Scott Wiener, included measures such as mandatory security tests for models with a cost of over $100 million and 'emergency stop buttons' to shut down models in emergency situations. Newsom argued that the bill's strict requirements were overly restrictive, even for models used in low-risk scenarios. However, this reasoning ignores the broader consequences of unregulated AI. AI security should not be treated as an afterthought or an optional consideration for economic growth. AI is a powerful technology that can cause significant harm if mismanaged, and the state's responsibility is to set a high standard for development. The veto of SB 1047 shows that California missed an opportunity to take a leadership role in AI governance at a critical time. Newsom's veto decision has implications beyond California, as it sends a signal that the US may be hesitant to implement strict AI security standards. This undermines the country's ability to lead in global AI governance. The failure of the bill also weakens international coordination on AI security. SB 1047 could have complemented federal efforts, such as the 'Safe and Reliable Artificial Intelligence Development and Use' executive order issued by the federal government in 2023, by providing a framework for consistency between state and federal regulations. The veto also undermines the need for proactive, implementable security measures. As AI becomes increasingly integrated into our lives, the need for such measures will only grow. California could have taken a leading role in establishing comprehensive AI security standards and serving as an example to the rest of the country and the world. Instead, the veto leaves a regulatory gap that could allow untested and potentially harmful technologies to spread unchecked.
Original language: tr
Publish date: October 01, 2024 04:16 AM
Source:[Haber3com](https://www.haber3.com/kose-yazisi/kaliforniyanin-yapay-zeka-guvenligi-konusundaki-kacirdigi-firsat-newsomun-vetosunun-sonucla/6203380)

**California's AI safety bill sparks heated debate as decision deadline nears**
California's Senate Bill (SB) 1047, the Safe and Secure Innovation for Frontier Artificial Intelligence Models Act, has sparked a heated debate among tech companies, politicians, and the entertainment industry. The bill, which requires large AI models to undergo safety tests before public release, has both supporters and critics. Supporters, including the National Organization for Women and the Future of Life Institute, argue that the bill is necessary to mitigate potential AI risks. However, tech giants like Google, Meta, and OpenAI, as well as various tech industry associations, oppose the bill, citing concerns about the potential burden of safety requirements on model developers. The bill's author, California State Senator Scott Wiener, has defended the legislation, arguing that it would only impact the largest AI developers and not small startups. As the decision deadline nears, the outcome of this bill could have far-reaching consequences for AI regulation across the United States.
Original language: en
Publish date: September 26, 2024 12:10 PM
Source:[CGTN](https://news.cgtn.com/news/2024-09-26/California-s-AI-safety-bill-sparks-heated-debate-1xdfYnLCI0M/p.html)

**S. Korea vows all-out efforts to become global AI powerhouse**
South Korea has vowed to make all-out efforts to become a global AI powerhouse, with President Yoon Suk Yeol setting a target for the country to become one of the world's top three AI leaders. According to Yoon, 'It is a time when AI determines national capabilities and economic growth, becoming a key factor in the economy and security.' The government and private sector will join hands to boost South Korea's AI capabilities, with a focus on establishing a national AI computing center, driving AI transformation across industries and society, and encouraging private sector investment in AI. Yoon also committed to bold reforms in intellectual property regulations and private protections to ensure they safeguard core values without becoming obstacles to innovation. The government has already launched the AI-Semiconductor Initiative, which includes plans to invest 9.4 trillion won (US$6.9 billion) in AI and AI chips by 2027 and create a 1.4 trillion-won fund to help the growth of AI chip companies. Additionally, South Korea and the United States have formed an AI working group and launched a global AI frontier lab for joint research and development between the two countries.
Original language: en
Publish date: September 26, 2024 11:59 AM
Source:[Naver](https://n.news.naver.com/mnews/article/001/0014949652)

**South Korean companies pledge 65 tln won in AI investment by 2027**
South Korean companies have pledged to invest 65 trillion won (US$48.9 billion) in artificial intelligence (AI) development by 2027 to compete with global leaders. The investment commitment was made during the inaugural meeting of the presidential committee on AI, which aims to create an AI strategy and coordinate AI research and development efforts. President Yoon called for collaborative efforts by the government and private sector to achieve the goal of becoming one of the top three AI leaders globally by 2027. According to Yoon, 'It is a time when AI determines national capabilities and economic growth, becoming a key factor in the economy and security.' The government plans to strengthen AI infrastructure by building a national AI computing center and expanding the capacity of graphic processing units (GPU). The government also plans to establish an AI Safety Institute and enact an AI act by the end of the year. Additionally, South Korea and the United States have formed an AI working group and launched a global AI frontier lab for joint research and development.
Original language: en
Publish date: September 26, 2024 11:42 AM
Source:[МИА Казинформ](https://en.inform.kz/news/south-korean-companies-pledge-65-tln-won-in-ai-investment-by-2027-0c798b)

**Korean gov't opens AI lab in New York with NYU**
South Korea has partnered with the United States to establish the Global AI Frontier Lab in New York, aiming to become a top-three player in the global artificial intelligence (AI) sector. The lab, a joint research initiative between the Korean government and New York University, will focus on foundational algorithms, trustworthy AI, and healthcare AI. The Korean government has invested 45 billion won ($34 million) and NYU has contributed 42 billion won by 2028. Minister of Science and ICT Yoo Sang-im emphasized that this is a significant moment marking a new turning point in AI cooperation and innovation between Korea and the United States. Professor Yann LeCun from NYU noted that the United States and Korea have the best research institutions across AI-related theories, algorithms, applications, hardware, and robotics. The Korean government also plans to open an AI hub in Seocho District, southern Seoul, and the AI Safety Research Institute to address issues related to deepfake crimes, fake news, and personal data breaches. The Presidential National AI Commission, set to launch in September 2024, will oversee AI-related policies and address potential AI-related side effects.
Original language: en
Publish date: September 25, 2024 11:07 AM
Source:[Naver](https://n.news.naver.com/mnews/article/009/0005369677)

**Korea and US Join Forces to Advance AI Research with Global AI Frontier Lab**
The Korean government has opened the Global AI Frontier Lab in New York in collaboration with New York University (NYU) to advance research in artificial intelligence (AI). The lab will focus on foundational algorithms, trustworthy AI, and healthcare AI, with a budget of 45 billion won ($34 million) from the Korean government and 42 billion won from NYU by 2028. The lab aims to bridge Korea-U.S. research and development, with joint directors Professor Yann LeCun from NYU and Professor Cho Kyung-hyun from NYU. Minister of Science and ICT Yoo Sang-im emphasized the significance of this collaboration, stating that it marks a new turning point in AI cooperation and innovation between Korea and the United States. The Korean government also plans to open an AI hub in Seoul and an AI Safety Research Institute to address issues related to deepfake crimes, fake news, and personal data breaches. The Presidential National AI Commission will be launched in September 2024 to oversee AI-related policies and address potential AI-related side effects. 'There is a need to swiftly resolve regulatory uncertainties to enable significant investments in AI technology,' said Ko Hwan-kyoung, a partner at Lee & Ko.
Original language: en
Publish date: September 25, 2024 02:06 AM
Source:[Maeil Business Newspaper](https://www.mk.co.kr/news/english/11124338)

**S. Korean Science Ministry Opens Joint AI Research Lab with NYU**
South Korea's science ministry has opened a joint AI research lab with New York University (NYU) to lead global-level AI research projects. The Global AI Frontier Lab at NYU will be co-led by professors Cho Kyung-hyun and Yann LeCun, and will focus on fundamental research in AI, trust and responsible AI, and AI for medical and health care. The lab will be funded by 45 billion won (US$33.8 million) from the science ministry over the next five years, with NYU contributing $31.5 million. Science Minister Yoo Sang-im said, 'This is a critical moment of change for the AI partnership between South Korea and the U.S.,' calling for further expansion of the bilateral partnership in science and technology.
Original language: en
Publish date: September 25, 2024 01:30 AM
Source:[Korea Bizwire](http://koreabizwire.com/s-korean-science-ministry-opens-joint-ai-research-lab-with-nyu/293247)

**S. Korean science ministry opens joint AI research lab with NYU**
South Korea's science ministry has opened a joint artificial intelligence (AI) research lab with New York University (NYU) to lead global-level AI research projects. The Global AI Frontier Lab at NYU aims to become a global AI joint research hub, co-led by professors Cho Kyung-hyun and Yann LeCun. The lab will focus on fundamental research in AI, studying trust and responsible AI, as well as AI for medical and health care. Science Minister Yoo Sang-im said, 'This is a critical moment of change for the AI partnership between South Korea and the U.S.,' calling for further expansion of the bilateral partnership in science and technology. The lab will be funded by 45 billion won (US$33.8 million) from the science ministry and $31.5 million from NYU over the next five years.
Original language: en
Publish date: September 24, 2024 10:00 PM
Source:[Naver](https://n.news.naver.com/mnews/article/001/0014946005)

**California's controversial AI bill's fate will be decided this week**
California Governor Gavin Newsom has until September 30 to decide the fate of the Safe and Secure Innovation for Frontier Artificial Intelligence Models Act, a bill that requires developers of large AI models to implement safety measures and submit to third-party audits. The bill's opponents in the tech industry argue that the safety requirements would shift the focus from improving AI to worrying about safety compliance, while Newsom stated, 'We've been working over the course of the last couple of years to come up with some rational regulation that supports risk-taking, but not recklessness.' The governor's office has been quiet on the bill, but Newsom's comments suggest that he is considering the bill's impact on California's competitive ecosystem.
Original language: en
Publish date: September 24, 2024 04:33 PM
Source:[Fast](https://www.fastcompany.com/91196018/california-ai-bill-fate-decided-this-week)

**Frontier Supercomputer: The Fastest in the World**
The Frontier supercomputer, located in Oak Ridge, Tennessee, has been the fastest in the world since its debut in 2022. It has 49,000 processors and can process data at a speed of over 100,000 laptops working simultaneously. According to Bronson Messer, director of science at the Oak Ridge National Laboratory, the Frontier consumes up to 27 megawatts of electricity, enough to power 10,000 homes. The supercomputer has been used for various research projects, including studies on subatomic particles, galaxies, and the development of new medicines. It has also been used to create large language models (LLM) for artificial intelligence (AI) applications. In 2023, the Frontier had 1,744 users from 18 countries, and it is expected that at least 500 scientific articles will be published based on its calculations in 2024. The supercomputer's main component is housed in a room the size of a warehouse, with 74 racks containing 9,408 nodes, each with four graphics processing units (GPU) and a central processing unit (CPU). The system is continuously evaluated by a team of engineers to detect any problems. To use the Frontier, scientists must personalize their code to use it efficiently, and only 25% of proposals are approved. The supercomputer is expected to be surpassed by the Aurora, located in the Argonne National Laboratory in Illinois, which is expected to enter operation by the end of 2024.
Original language: es
Publish date: September 23, 2024 09:40 AM
Source:[La Tercera](https://www.latercera.com/tendencias/noticia/como-es-el-supercomputador-mas-rapido-del-mundo-y-cuales-son-sus-funciones/TJH4PJFJONE65EAEGRVTYXC3NU)

**Why Some Are Predicting That Governor Newsom Is Going To Veto That AI ‘Existential Risk' Bill Pending In California**
California Governor Newsom is deciding whether to sign or veto the 'Safe and Secure Innovation for Frontier Artificial Intelligence Models Act' (SB 1047), a bill aimed at regulating AI. The bill has garnered significant attention due to its sweeping attempt to encompass legally binding AI governance constraints. Governor Newsom has expressed concerns about the bill, stating 'What are the demonstrable risks in AI and what are the hypothetical risks? I can't solve for everything.' The bill's fate is uncertain, with both supporters and opponents weighing in. If vetoed, the California legislature can claim they did what was needed to push forward on AI legislation, and Congress can be blamed for not taking action. The bill's future is uncertain, with September 30, 2024, being the deadline for Governor Newsom's decision.
Original language: en
Publish date: September 22, 2024 10:39 PM
Source:[Forbes](https://www.forbes.com/sites/lanceeliot/2024/09/22/why-some-are-predicting-that-governor-newsom-is-going-to-veto-that-ai-existential-risk-bill-pending-in-california)

**Clear as mud: global rules around AI are starting to take shape but remain a little fuzzy**
The global landscape of AI regulation is fragmented and unclear, with multiple governments and organizations pushing for their own rules and restrictions. The US, EU, and individual states are all working to establish limits on AI, but the patchwork of regulations makes it challenging for security leaders to stay on top of the requirements. The US is pushing for an international consensus on AI, while the EU is advancing its own legislation on 'worldwide rules' concerning AI. However, there are still areas of fuzziness in AI rules, particularly in the EU's efforts to categorize high- and low-risk systems. Mike Price, chief technology officer at ZeroFox, emphasized the need for a strategy that protects against potential misuse and promotes the development of a stronger and more sustainable global AI ecosystem. He also highlighted the importance of education around AI, as current capabilities favor bad actors and leave the uneducated vulnerable to attack. The US is also experiencing a fragmented nature of AI regulation, with individual states deriving their own regulations and putting in place their guard rails. California's legislation, the Safe and Secure Innovation for Frontier Artificial Intelligence Models Act, is a notable example, mandating safety testing and empowering the attorney general to take companies to task if their AI creates a threat.
Original language: en
Publish date: September 21, 2024 12:30 AM
Source:[CSO Online](https://www.csoonline.com/article/3535621/clear-as-mud-global-rules-around-ai-are-starting-to-take-shape-but-remain-a-little-fuzzy.html)

**Will California pass an AI law that is dividing Silicon Valley?**
California's Governor Gavin Newsom has until September 30th to decide whether to sign a Bill regulating artificial intelligence (AI), which has divided Silicon Valley. The Bill, known as the Safe and Secure Innovation for Frontier Artificial Intelligence Systems Act, aims to require developers of large AI models to perform basic safety testing, assess potential risks, and implement reasonable safeguards. Critics argue that the Bill would hobble early-stage companies and open-source developers, while proponents say it is necessary to prevent harm and ensure accountability. The Bill would apply to companies doing business in California, regardless of their location, and would introduce civil penalties of up to 10% of the cost of training a model for developers whose tools cause harm. Supporters, including actor's union SAG-AFTRA and women's groups, have expressed qualified support for the Bill, while opponents, including OpenAI and venture capital firm Andreessen Horowitz, have warned that it would stifle innovation and lead to a brain drain in California. 'The big AI companies which have been the most vocal on this issue are currently locked in their race for market share and profit maximisation, which can lead to cutting corners when it comes to safety, and that's why we need some rules for those leading this race,' said Yoshua Bengio, a professor at the University of Montreal. 'Philosophically, anticipating the consequences of how people are going to use your code in software is a very difficult problem. How will people use it, how will you anticipate that somebody will do harm? It's a great inhibitor. It's a very slippery slope,' said Dario Gil, director of research at IBM. 'I would love for this to be federal legislation: if Congress were to act in this space and pass a strong AI safety Bill I'd be happy to pack up and go home,' said state senator Scott Wiener. 'But the sad reality is that while Congress has been very, very successful on healthcare, infrastructure and climate, it's really struggled with technology regulation... Until Congress acts, California has an obligation to lead because we are the heartland of the tech industry.'
Original language: en
Publish date: September 19, 2024 04:31 AM
Source:[The Irish Times](https://www.irishtimes.com/business/2024/09/19/the-ai-bill-driving-a-wedge-through-silicon-valley/)

**$200 million supercomputer will be scrapped within weeks — ORNL's Summit could be a bargain for someone with very deep pockets, lofty ambitions (and some serious real estate to park it all)**
The Oak Ridge National Laboratory (ORNL) has announced plans to retire its Summit supercomputer in November 2024, after six years of service and over 200 million node hours of research. Summit, which was the world's most powerful supercomputer in 2018, has been surpassed by newer technologies and now ranks ninth globally. Its successor, Frontier, has claimed the title of the world's first exascale supercomputer and significantly outpaces Summit's computational power. ORNL has not confirmed whether it would consider selling Summit, but acquiring the supercomputer could be an attractive opportunity for those with the financial resources. The supercomputing landscape has evolved massively since Summit's debut, with the rise of AI-driven data centers increasing demand on energy resources. Frontier is both the world's fastest and most energy-efficient supercomputer, but ORNL is planning a successor, named Discovery, which is set to debut by 2028. 'Summit has played a key role in over 100 research projects, including contributions to the National AI Research Resource initiative,' said ORNL, as they bid farewell to the supercomputer. 'There is certainly a precedent for such systems being sold,' said ORNL, referencing the sale of the Cheyenne supercomputer for $480,085. However, Summit would likely come with a much higher price tag. 'The supercomputing landscape has evolved massively since Summit's debut, especially with the rise of AI-driven data centers,' said ORNL. 'This shift has increased demand on energy resources, with data center electricity consumption projected to reach 6.8% of total US power generation by 2030, up from 4% today.'
Original language: en
Publish date: September 18, 2024 05:36 PM
Source:[MSN](https://www.msn.com/en-us/news/technology/200-million-supercomputer-will-be-scrapped-within-weeks-ornl-s-summit-could-be-a-bargain-for-someone-with-very-deep-pockets-lofty-ambitions-and-some-serious-real-estate-to-park-it-all/ar-AA1qNoSA)

**Is Spatial AI the Next Frontier in Machine Learning Development?**
World Labs, a startup founded by AI researcher Fei-Fei Li and a team of computer vision experts, has raised over $230 million to develop AI models with spatial intelligence capabilities. The company aims to 'lift AI models from the 2D plane of pixels to full 3D worlds — both virtual and real — endowing them with spatial intelligence as rich as our own.' World Labs plans to develop large world models (LWMs) that can perceive, generate and interact with 3D environments, with potential applications in fields ranging from entertainment to urban planning. The funding round was led by venture capital firms and included investments from tech industry leaders such as Salesforce CEO Marc Benioff and Google's Jeff Dean. The development of spatial AI could have implications for numerous fields, including architecture, robotics, and entertainment, and is transforming industries by integrating geographic data with AI for improved decision making and operational efficiency.
Original language: en
Publish date: September 16, 2024 11:04 PM
Source:[PYMNTS](https://www.pymnts.com/artificial-intelligence-2/2024/is-spatial-ai-the-next-frontier-in-machine-learning-development)

**The AI bill driving a wedge through Silicon Valley**
California's proposed AI bill, the Safe and Secure Innovation for Frontier Artificial Intelligence Systems Act (SB 1047), aims to regulate the development of artificial intelligence (AI) in the state. The bill, which has been amended to address concerns from critics, would require developers of large AI models to perform basic safety testing, assess potential risks, and implement reasonable safeguards. It would also introduce civil penalties for developers whose tools cause harm and create liabilities for companies offering computing resources to train AI models. Critics argue that the bill would hobble early-stage companies and stifle innovation, while supporters say it is necessary to address the risks associated with AI. Governor Gavin Newsom has until September 30 to decide whether to sign the bill into law. 'The big AI companies which have been the most vocal on this issue are currently locked in their race for market share and profit maximisation, which can lead to cutting corners when it comes to safety, and that's why we need some rules for those leading this race,' said Yoshua Bengio, a professor at the University of Montreal. 'Philosophically, anticipating the consequences of how people are going to use your code in software is a very difficult problem. How will people use it, how will you anticipate that somebody will do harm? It's a great inhibitor. It's a very slippery slope,' said Dario Gil, director of research at IBM. 'I would love for this to be federal legislation: if Congress were to act in this space and pass a strong AI safety bill I'd be happy to pack up and go home,' said Scott Wiener, the state senator who introduced the bill. 'But the sad reality is that while Congress has been very, very successful on healthcare, infrastructure and climate, it's really struggled with technology regulation . . . Until Congress acts, California has an obligation to lead because we are the heartland of the tech industry.' 
Original language: en
Publish date: September 12, 2024 10:02 AM
Source:[Financial Times News](https://www.ft.com/content/c5e7bf0b-cadc-4673-b2d8-daa9d89f2230)

**Examining the capabilities and risks of advanced AI systems**
The first global AI Safety Summit in the UK in November 2023 brought together world leaders, technologists, and civil society groups to discuss the safe and responsible development of frontier AI. The summit produced the Bletchley Declaration on AI safety and commissioned a 'State of the Science' report to assess the present and potential future risks, opportunities, and capabilities of general-purpose AI. The report highlights the need for broader global cooperation in defining AI risks and developing robust mitigation solutions. It also emphasizes the importance of equitable inclusion of diverse perspectives, particularly from the Global South, to contend with the risks and harms from AI and democratize the benefits of these systems. The report notes that harmful bias and underrepresentation in AI systems have been challenges since before the current AI boom and will likely persist for the foreseeable future. It also warns that the global AI divide, where frontier AI research and development primarily occurs in Western countries, will exacerbate inequalities between nations. The report concludes that the capabilities of general-purpose AI are advancing rapidly, but there is limited understanding of the capabilities and inner workings of these systems, especially as they are suited to generate external harms. Experts disagree on the expected pace of future progress of general-purpose AI capabilities, and several technical methods can help to mitigate risks, though all current methods have limitations.
Original language: en
Publish date: September 10, 2024 06:58 PM
Source:[The Brookings Institution](https://www.brookings.edu/articles/examining-advanced-ai-capabilities-and-risks)

**US proposes reporting mandates for AI developers, cloud providers**
The US Commerce Department is proposing a reporting requirement for leading AI developers and cloud providers to evaluate their technology's safety and defense capabilities. The proposed rule would mandate 'frontier' AI model and computing clusters to provide detailed reporting about their developmental activities and cybersecurity measures. Commerce Secretary Gina Raimondo noted that AI is 'progressing rapidly' with both 'tremendous promise and risk.' The information collected will be vital for ensuring these technologies meet stringent standards for safety and reliability, can withstand cyberattacks, and have limited risk of misuse by foreign adversaries or non-state actors. This proposal follows a pilot survey of AI developers and a wider push from the federal government to better understand the capabilities and risks of AI. 
Original language: en
Publish date: September 09, 2024 03:09 PM
Source:[AOL](https://www.aol.com/us-proposes-reporting-mandates-ai-150940057.html)

*Generated by AI at [AskNews](https://asknews.app), check out the [API](https://docs.asknews.app) for more information*.
